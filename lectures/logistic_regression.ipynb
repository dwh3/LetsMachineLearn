{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "\n",
    "### What is it?\n",
    "Models $y$ as a linear function of $x$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm  # For the regression line\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 1000  # number of observations\n",
    "\n",
    "# Generate random heights (in inches)\n",
    "heights = np.random.normal(loc=66, scale=4, size=n)\n",
    "\n",
    "# Generate weights (in pounds) with a linear relationship to height\n",
    "epsilon = np.random.normal(loc=0, scale=10, size=n)  # Error term\n",
    "beta0 = -120  # Intercept parameter (base weight)  Note:  Often needs adjustment\n",
    "beta1 = 4.5  # Slope parameter (weight per inch)   Note:  Often needs adjustment\n",
    "\n",
    "weights = beta0 + beta1 * heights + epsilon\n",
    "\n",
    "# Create data frame\n",
    "df = pd.DataFrame({'Height': heights, 'Weight': weights})\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of the data\n",
    "ax.scatter(df['Height'], df['Weight'], alpha=0.6, s=20)  # Adjust 's' for point size\n",
    "\n",
    "# Add regression line (using statsmodels for a clean fit)\n",
    "model = sm.ols(\"Weight ~ Height\", data=df).fit()\n",
    "predictions = model.predict(df)\n",
    "ax.plot(df['Height'], predictions, color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Height (inches)\")\n",
    "ax.set_ylabel(\"Weight (pounds)\")\n",
    "ax.set_title(\"Height vs Weight\")\n",
    "\n",
    "# Customize appearance (optional, but good practice)\n",
    "ax.grid(True, linestyle=':', alpha=0.6)  # Add a subtle grid\n",
    "# ax.set_facecolor('#f0f0f0') # Optional: Light gray background\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Alternative Plotting (using Seaborn, even simpler) ---\n",
    "# import seaborn as sns\n",
    "# sns.regplot(x='Height', y='Weight', data=df, ci=None, scatter_kws={'alpha':0.6}, line_kws={'color':'blue'})\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Limitation\n",
    "\n",
    "But we didn’t impose a restriction on the values that $y$ can take, meaning it can range from $- \\infty$ to $\\infty$. But the answer to some questions cannot take just any value.\n",
    "\n",
    "\n",
    "For example, we can try to predict the probability of something, such as if a person is obese.\n",
    "\n",
    "\n",
    "\n",
    "$$P(obese=1 | X)=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 1000  # number of observations\n",
    "\n",
    "# Generate random weights (in pounds)\n",
    "weights = np.random.normal(loc=160, scale=50, size=n)\n",
    "\n",
    "# Generate binary obesity status (1 for obese, 0 for not obese)\n",
    "obesity_threshold = 200\n",
    "# Logistic function for probability of obesity\n",
    "prob_obese = 1 / (1 + np.exp(-(weights - obesity_threshold) / 10))\n",
    "obesity_status = np.random.binomial(1, prob_obese) # n=1, size= prob_obese\n",
    "\n",
    "# Create data frame\n",
    "df = pd.DataFrame({'Weight': weights, 'Obese': obesity_status})\n",
    "\n",
    "# Fit a linear regression model\n",
    "lm_model = sm.ols(\"Obese ~ Weight\", data=df).fit()\n",
    "df['Predicted'] = lm_model.predict(df)\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot with jitter (using numpy for jitter)\n",
    "ax.scatter(df['Weight'], df['Obese'] + np.random.uniform(-0.05, 0.05, size=n),  # Add jitter\n",
    "           alpha=0.6, s=20)\n",
    "\n",
    "# Add the *linear* regression line (from statsmodels)\n",
    "ax.plot(df['Weight'], df['Predicted'], color='red', linewidth=2)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Weight (pounds)\")\n",
    "ax.set_ylabel(\"Obesity Status (1 = Obese, 0 = Not Obese)\")\n",
    "ax.set_title(\"Weight vs Obesity Status with Linear Regression\")\n",
    "\n",
    "# Extend y-axis limits\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "\n",
    "# Customize appearance (optional)\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Alternative with Seaborn (for comparison) ---\n",
    "# import seaborn as sns\n",
    "# fig, ax = plt.subplots(figsize=(8,6))\n",
    "# sns.regplot(x='Weight', y='Obese', data=df, ci=None,\n",
    "#             scatter_kws={'alpha':0.6, 's':20,  'y_jitter':0.05},\n",
    "#             line_kws={'color':'red'},\n",
    "#             logistic=False, #  Important:  Keep this as FALSE for *linear* regression.\n",
    "#             ax=ax) #Pass the axis object.\n",
    "# ax.set_ylim(-0.5, 1.5)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "So what is the problem here? Well a linear regression could give a predicted probability less than 0, or greater than 1, which does not make any sense. What would make more sense then? Well something that keeps the bounds between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 1000  # number of observations\n",
    "\n",
    "# Generate random weights (in pounds)\n",
    "weights = np.random.normal(loc=160, scale=50, size=n)\n",
    "\n",
    "# Generate binary obesity status (1 for obese, 0 for not obese)\n",
    "obesity_threshold = 200\n",
    "# Logistic function for probability of obesity\n",
    "prob_obese = 1 / (1 + np.exp(-(weights - obesity_threshold) / 10))\n",
    "obesity_status = np.random.binomial(1, prob_obese)\n",
    "\n",
    "# Create data frame\n",
    "df = pd.DataFrame({'Weight': weights, 'Obese': obesity_status})\n",
    "\n",
    "# Fit a *logistic* regression model (using statsmodels.formula.api)\n",
    "logit_model = sm.logit(\"Obese ~ Weight\", data=df).fit()  # Use sm.logit, not sm.ols\n",
    "df['Predicted'] = logit_model.predict(df)  # Get predicted probabilities\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot with jitter\n",
    "ax.scatter(df['Weight'], df['Obese'] + np.random.uniform(-0.05, 0.05, size=n),\n",
    "           alpha=0.6, s=20)\n",
    "\n",
    "# Add the *logistic* regression curve (using the predicted probabilities)\n",
    "# Sort the x-values for a smooth curve\n",
    "df_sorted = df.sort_values('Weight')\n",
    "ax.plot(df_sorted['Weight'], df_sorted['Predicted'], color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Weight (pounds)\")\n",
    "ax.set_ylabel(\"Obesity Status (1 = Obese, 0 = Not Obese)\")\n",
    "ax.set_title(\"Weight vs Obesity Status with Logistic Regression\")\n",
    "\n",
    "# Extend y-axis limits (optional, but good for showing probabilities)\n",
    "ax.set_ylim(-0.1, 1.1)  # Show the full 0-1 probability range\n",
    "\n",
    "# Customize appearance (optional)\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Alternative Plotting (using Seaborn) ---\n",
    "# import seaborn as sns\n",
    "# fig, ax = plt.subplots(figsize = (8,6))\n",
    "# sns.regplot(x='Weight', y='Obese', data=df, ci=None,\n",
    "#             scatter_kws={'alpha':0.6, 's':20, 'y_jitter':0.05},\n",
    "#             line_kws={'color':'blue'},\n",
    "#             logistic=True, #  *Important*: Set logistic=True for logistic regression.\n",
    "#             ax=ax)\n",
    "# ax.set_ylim(-0.1, 1.1)\n",
    "# plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Logit Derivation\n",
    "\n",
    "\n",
    "\n",
    "So what would we rather have happen. Well what we really want is:\n",
    "\n",
    "- To predict how a change in $x$ causes a change in the probability that $y=1$.\n",
    "- To ensure that even if we do not restrict what values $x$ can take, we have values of $y$ that are possible.\n",
    "\n",
    "\n",
    "\n",
    "Well even if the probability of y is limited between 0 and 1, we can look at a function of the probability, and this function would not have the same limits, meaning it could range from $- \\infty$ to $\\infty$ as we had before.\n",
    "\n",
    "\n",
    "\n",
    "$$f(P(obese=1 | X))=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Odds\n",
    "\n",
    "\n",
    "So instead of looking at the probability, let's see what is known as the **odds**, which is defined at the probability of something happening, divided by the probability of it not happening:\n",
    "\n",
    "\n",
    "$$\\frac{P(y=1)}{P(y=0)}$$\n",
    "\n",
    "And if there are only two outcomes, is the same as:\n",
    "\n",
    "\n",
    "$$\\frac{P(y=1)}{1-P(y=1)}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So if $P(y=1)=0.75$, then $P(y=0)=1-P(y=1)=1-0.75=025$, then the odds are $$\\frac{P(y=1)}{1-P(y=1)}=\\frac{0.75}{0.25}=1$$\n",
    "\n",
    "Or we would say the odds are three to one (3:1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So what happens at the extremes? Well, let's consider the case where the probability is essentially 1, let's say that Mike Tyson would beat me in a fight, well then the odds are:\n",
    "\n",
    "\n",
    "$$\\frac{P(y=1)}{1-P(y=1)}$$\n",
    "\n",
    "$$\\lim_{P(y=1) \\to 1} \\frac{P(y=1)}{1 - P(y=1)}=\\frac{1}{0}=\\infty$$\n",
    "\n",
    "\n",
    "And on the converse, if we consider something that will not happen, such as the sun exploding today, then the odds are:\n",
    "\n",
    "\n",
    "$$\\lim_{P(y=1) \\to 0} \\frac{P(y=1)}{1 - P(y=1)}=\\frac{0}{1}=0$$\n",
    "\n",
    "\n",
    "\n",
    "This means that regardless of the range of x, the range of the odds are brounded by 0 and infinity. This is a step in the right direction, since it removed the upper bound of 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Log Odds\n",
    "\n",
    "\n",
    "So since that removed the upper bound, we want a function of this which removes the lower bound. Well thankfully we have a well known function known as the log. This has two interesting properties. \n",
    "\n",
    "$$log(0)=-\\infty$$\n",
    "\n",
    "And \n",
    "\n",
    "\n",
    "$$log(\\infty )=\\infty$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Which means that if we take the log of our odds, this function now has no bounds:\n",
    "\n",
    "\n",
    "\n",
    "$$\\lim_{P(y=1) \\to 1} log(\\frac{P(y=1)}{1 - P(y=1)})=\\infty$$\n",
    "\n",
    "\n",
    "And\n",
    "\n",
    "\n",
    "$$\\lim_{P(y=1) \\to 0} log(\\frac{P(y=1)}{1 - P(y=1)})=-\\infty$$\n",
    "\n",
    "\n",
    "\n",
    "That means that since we run into problems with estimating the following, because of the constraints of y:\n",
    "\n",
    "\n",
    "\n",
    "$$P(y=1 | X)=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n",
    "\n",
    "We can instead remove these constraints and instead estimate:\n",
    "\n",
    "$$log(\\frac{P(y=1)}{1 - P(y=1)})=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This is what is known as the logistic regression model.** Specifically the logit model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "While this is the functional form, the **estimation is done with Maximum Liklihood Estimation**, and the derivation of that is fairly involved, and we probably do not want to go through that, so we can assume the statistical software does that estimation correctly. But given that, we can look at how we can interpret those coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting coefficients\n",
    "\n",
    "\n",
    "In our linear regressions before, the interpretation of the regression coefficient would mean that one change in variable X, would result in a $\\beta$ change in y. Or in the following, we would expect someone to gain 2.5 pounds for each inch of height.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm  # For the regression line\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 1000  # number of observations\n",
    "\n",
    "# Generate random heights (in inches)\n",
    "heights = np.random.normal(loc=66, scale=4, size=n)\n",
    "\n",
    "# Generate weights (in pounds) with a linear relationship to height\n",
    "epsilon = np.random.normal(loc=0, scale=10, size=n)  # Error term\n",
    "beta0 = -120  # Intercept parameter (base weight)  Note:  Often needs adjustment\n",
    "beta1 = 4.5  # Slope parameter (weight per inch)   Note:  Often needs adjustment\n",
    "\n",
    "weights = beta0 + beta1 * heights + epsilon\n",
    "\n",
    "# Create data frame\n",
    "df = pd.DataFrame({'Height': heights, 'Weight': weights})\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot of the data\n",
    "ax.scatter(df['Height'], df['Weight'], alpha=0.6, s=20)  # Adjust 's' for point size\n",
    "\n",
    "# Add regression line (using statsmodels for a clean fit)\n",
    "model = sm.ols(\"Weight ~ Height\", data=df).fit()\n",
    "predictions = model.predict(df)\n",
    "ax.plot(df['Height'], predictions, color='blue', linewidth=2)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Height (inches)\")\n",
    "ax.set_ylabel(\"Weight (pounds)\")\n",
    "ax.set_title(\"Height vs Weight\")\n",
    "\n",
    "# Customize appearance (optional, but good practice)\n",
    "ax.grid(True, linestyle=':', alpha=0.6)  # Add a subtle grid\n",
    "# ax.set_facecolor('#f0f0f0') # Optional: Light gray background\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Alternative Plotting (using Seaborn, even simpler) ---\n",
    "# import seaborn as sns\n",
    "# sns.regplot(x='Height', y='Weight', data=df, ci=None, scatter_kws={'alpha':0.6}, line_kws={'color':'blue'})\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "But what would the interpretation be here? Well a 1 unit change in x would be associated with a $\\beta$ change in the log odds. This is kind of unnecessary to understand more than this, but a good rule of thumb is:\n",
    "\n",
    "\n",
    "- If $\\beta$ is positive, \n",
    "  - when $x \\uparrow$ then $P(y=1) \\uparrow$\n",
    "  - when $x \\downarrow$ then $P(y=1) \\downarrow$\n",
    "  \n",
    "- If $\\beta$ is negative, \n",
    "  - when $x \\uparrow$ then $P(y=1) \\downarrow$\n",
    "  - when $x \\downarrow$ then $P(y=1) \\uparrow$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Predicting probability\n",
    "\n",
    "\n",
    "Since the logistic regression is of the form:\n",
    "\n",
    "\n",
    "$$log(\\frac{P(y=1)}{1 - P(y=1)})=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n",
    "\n",
    "Once we run the logistic regression in whatever software we use, we will get estimates for all $\\beta$'s. We could then predict the probability of that outcome occuring, given only their observed values. But how?\n",
    "\n",
    "\n",
    "This is a little more straight forward than interpreting the coeffiecients themselves.\n",
    "\n",
    "If we want the predicted probability, we can do some algebra on our original function, and get:\n",
    "\n",
    "\n",
    "$$log(\\frac{P(y=1)}{1 - P(y=1)})=\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon$$\n",
    "\n",
    "\n",
    "$$e^{log(\\frac{P(y=1)}{1 - P(y=1)})}=e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}$$\n",
    "\n",
    "\n",
    "$$\\frac{P(y=1)}{1 - P(y=1)}=e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$P(y=1)=e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}(1 - P(y=1))$$\n",
    "\n",
    "$$P(y=1)=e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon} - P(y=1) \\times e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}$$\n",
    "\n",
    "divide both sides by $P(y=1)$\n",
    "\n",
    "$$1=\\frac{e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}}{P(y=1)} -e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}$$\n",
    "\n",
    "$$1+e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}=\\frac{e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}}{P(y=1)}$$\n",
    "$$\\frac{1+e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}}{e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots +\\epsilon}}=\\frac{1}{P(y=1)}$$\n",
    "$$\\frac{e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots}}{1+e^{\\beta_0+\\beta_1 x_1 +\\beta_2 x_2 \\cdots}}=P(y=1)$$\n",
    "\n",
    "## Properties of the predicted probability\n",
    "\n",
    "\n",
    "We can plot the predicted probability as a function of x, as:\n",
    "\n",
    "$$\\frac{e^{x}}{1+e^{x}}=P(y=1)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define the logistic (sigmoid) function\n",
    "def logistic_function(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Create a sequence of values from -10 to 10\n",
    "x_values = np.arange(-10, 10.1, 0.1)  # Use arange for floating-point step\n",
    "# Calculate the logistic function for each value\n",
    "y_values = logistic_function(x_values)\n",
    "\n",
    "# Create a data frame for plotting (optional, but good practice)\n",
    "logistic_df = pd.DataFrame({'x': x_values, 'y': y_values})\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the logistic function\n",
    "ax.plot(logistic_df['x'], logistic_df['y'], color='blue', linewidth=2)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"Predicted Probability P(y=1)\")\n",
    "ax.set_title(\"Sigmoid Function\")\n",
    "\n",
    "# Extend y-axis limits slightly\n",
    "ax.set_ylim(-0.1, 1.1)\n",
    "\n",
    "# Customize appearance (optional)\n",
    "ax.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Demonstrate the behavior for very low and very high values\n",
    "low_value = -100\n",
    "high_value = 100\n",
    "low_prob = logistic_function(low_value)\n",
    "high_prob = logistic_function(high_value)\n",
    "\n",
    "print(f\"When the exponent is very low (e.g., {low_value}), the predicted probability is approximately {low_prob:.8f}\")  # Use f-string for formatting\n",
    "print(f\"When the exponent is very high (e.g., {high_value}), the predicted probability is approximately {high_prob:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Benefits\n",
    "\n",
    "\n",
    "- Limits range of predicted probability between 0 and 1.\n",
    "\n",
    "- Predicted impact of $x_i$ is conditional on $x_j$.\n",
    "\n",
    "- Could give classification ranges based on predicted probability.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Use cases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "\n",
    "Let’s manually compute the probability of smoking for (i) all workers, (ii) workers affected by workplace smoking bans, and, (iii) workers not affected by workplace smoking bans.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf  # For a closer equivalent to AER's data loading\n",
    "import numpy as np\n",
    "\n",
    "# Load the SmokeBan data (using statsmodels, which includes it)\n",
    "#  AER's `data(SmokeBan)` loads it into the environment.\n",
    "#  statsmodels makes datasets available via .get_rdataset()\n",
    "try:\n",
    "    data = sm.datasets.get_rdataset(\"SmokeBan\", \"AER\").data\n",
    "except AttributeError:  # Handle older statsmodels versions\n",
    "    #   In older statsmodels versions, you load data differently\n",
    "    data = smf.datasets.get_rdataset(\"SmokeBan\", \"AER\", cache=True).data\n",
    "\n",
    "\n",
    "# Creating binary version of smoker\n",
    "data['binarySmoker'] = (data['smoker'] == 'yes').astype(int)\n",
    "\n",
    "# Probability of smoking of all workers\n",
    "pSmokingAll = data['binarySmoker'].mean()\n",
    "\n",
    "# Probability of smoking of workers with ban\n",
    "pSmokingBan = data.loc[data['ban'] == 'yes', 'binarySmoker'].mean()\n",
    "\n",
    "# Probability of smoking of workers with no ban\n",
    "pSmokingNoBan = data.loc[data['ban'] == 'no', 'binarySmoker'].mean()\n",
    "\n",
    "\n",
    "# Create a DataFrame for plotting (like in R)\n",
    "pSmoking = pd.DataFrame({\n",
    "    'y': [pSmokingNoBan, pSmokingAll, pSmokingBan],\n",
    "    'ban': [\"With No Ban\", \"All Workers\", \"With Ban\"]\n",
    "})\n",
    "\n",
    "# --- Plotting ---\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Bar plot (using matplotlib directly)\n",
    "ax.bar(pSmoking['ban'], pSmoking['y'], color=\"#1f77b4\")  # Access columns by name\n",
    "#Note, the default matplotlib color is a good choice.\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Ban Status\")  # More descriptive x-label\n",
    "ax.set_ylabel(\"Proportion of Smokers\")\n",
    "ax.set_title(\"Smoking Rates by Workplace Ban Status\")\n",
    "\n",
    "\n",
    "# Customize appearance (optional)\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.6)  # Add a subtle grid\n",
    "ax.spines['top'].set_visible(False)       # Remove top spine\n",
    "ax.spines['right'].set_visible(False)     # Remove right spine\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- Alternative using Seaborn (more concise)\n",
    "# import seaborn as sns\n",
    "# fig, ax = plt.subplots(figsize=(8, 6))\n",
    "# sns.barplot(x='ban', y='y', data=pSmoking, ax=ax, order=[\"With No Ban\", \"All Workers\", \"With Ban\"], palette=\"deep\")\n",
    "# ax.set_xlabel(\"Ban Status\")\n",
    "# ax.set_ylabel(\"Proportion of Smokers\")\n",
    "# ax.set_title(\"Smoking Rates by Workplace Ban Status\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This suggests approximately a 10 percentage point decrease in the probability of smoking if there is a smoking ban.\n",
    "\n",
    "\n",
    "But we can be even more precise with a regression. Here we can even run a linear model and a logit regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "from statsmodels.iolib.summary2 import summary_col\n",
    "\n",
    "# Load the SmokeBan data\n",
    "try:\n",
    "    data = sm.datasets.get_rdataset(\"SmokeBan\", \"AER\").data\n",
    "except AttributeError:\n",
    "    data = smf.datasets.get_rdataset(\"SmokeBan\", \"AER\", cache=True).data\n",
    "\n",
    "# Convert 'education' to categorical and set 'hs drop out' as reference\n",
    "data['education'] = pd.Categorical(data['education'])\n",
    "data['education'] = data['education'].cat.reorder_categories(\n",
    "    ['hs drop out', 'hs', 'some college', 'college', 'master'], ordered=True\n",
    ")\n",
    "\n",
    "# Creating binary version of smoker\n",
    "data['binarySmoker'] = (data['smoker'] == 'yes').astype(int)\n",
    "\n",
    "# Estimating linear probability model\n",
    "linear_model = smf.ols(\"binarySmoker ~ ban + age + I(age**2) + education + afam + hispanic + gender\", data=data).fit()\n",
    "\n",
    "# Estimating logit model\n",
    "logit_model = smf.logit(\"binarySmoker ~ ban + age + I(age**2) + education + afam + hispanic + gender\", data=data).fit()\n",
    "\n",
    "# --- Dynamically Generate Covariate Labels ---\n",
    "variable_labels = []\n",
    "base_labels = {  #  Base labels *without* the level\n",
    "    'ban[T.yes]': \"Ban (yes = 1)\",  # Explicitly handle the 'ban' variable\n",
    "    'age': \"Age\",\n",
    "    'I(age ** 2)': \"Age^2\",\n",
    "    'education': \"Education\",  # Base label for education\n",
    "    'afam[T.yes]': \"African-American (yes = 1)\",  # Explicitly handle afam\n",
    "    'hispanic[T.yes]': \"Hispanic (yes = 1)\",  # Explicitly handle hispanic\n",
    "    'gender[T.female]': \"Gender (Female = 1)\"   # Explicitly handle gender\n",
    "}\n",
    "\n",
    "for name in linear_model.params.index:  # Iterate through *actual* coefficient names\n",
    "    if name in base_labels:\n",
    "        variable_labels.append(base_labels[name])\n",
    "    elif name.startswith('education'):\n",
    "        # Extract the education level from the coefficient name\n",
    "        level = name.replace('education[T.', '').replace(']', '')\n",
    "        # Construct the label dynamically\n",
    "        variable_labels.append(f\"Education ({level} = 1)\")\n",
    "    else:\n",
    "        variable_labels.append(name) #Keep original in this scenario\n",
    "\n",
    "# --- Create the summary table ---\n",
    "\n",
    "results_table = summary_col(\n",
    "    [linear_model, logit_model],\n",
    "    stars=True,\n",
    "    float_format='%0.3f',\n",
    "    model_names=['Linear', 'Logit'],\n",
    "    info_dict={\n",
    "        'N': lambda x: f\"{x.nobs:.0f}\",\n",
    "        'R2': lambda x: f\"{x.rsquared:.3f}\" if hasattr(x, 'rsquared') else f\"{x.prsquared:.3f}\"\n",
    "    },\n",
    "     regressor_order=linear_model.params.index.tolist(), #Ensure correct order\n",
    "    drop_omitted=True\n",
    ")\n",
    "\n",
    "# NO MORE MANUAL LABEL REPLACEMENT\n",
    "\n",
    "# Display output as HTML\n",
    "HTML(results_table.as_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Interpretation\n",
    "\n",
    "- Both models predict that the smoking ban reduces the probability of smoking. And the effect is statistically different than zero.\n",
    "\n",
    "- The linear probability model predicts that the reduction in probability of being a smoker caused by the smoking ban is 5 percent.\n",
    "\n",
    "- The logit model predicts that the odds of being a smoker relative to not being a smoker are change by a factor of $e^{\\beta}\\approx 0.7695$\n",
    "\n",
    "- In the linear regression model, the effect is always the same, regardless of the value of the other control variables.\n",
    "\n",
    "\n",
    "In the logistic model, the effect of the x variable depends on the value of the other control variables. For instance, consider that we have two individuals that are: 30 years old, with a college degree, that is not African-american, and not Hispanic, but one is male and the other is female. Do you think the effect of the ban will be the same for both genders?\n",
    "\n",
    "\n",
    "To answer this question we can use the invlogit function to get our predicted probability of smoking for different individuals:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load the SmokeBan data\n",
    "try:\n",
    "    data = sm.datasets.get_rdataset(\"SmokeBan\", \"AER\").data\n",
    "except AttributeError:\n",
    "    data = smf.datasets.get_rdataset(\"SmokeBan\", \"AER\", cache=True).data\n",
    "\n",
    "# Convert 'education' to categorical and set 'hs drop out' as reference\n",
    "data['education'] = pd.Categorical(data['education'])\n",
    "data['education'] = data['education'].cat.reorder_categories(\n",
    "    ['hs drop out', 'hs', 'some college', 'college', 'master'], ordered=True\n",
    ")\n",
    "# Creating binary version of smoker\n",
    "data['binarySmoker'] = (data['smoker'] == 'yes').astype(int)\n",
    "\n",
    "# Estimating logit model  (we need the fitted model from before)\n",
    "logit_model = smf.logit(\"binarySmoker ~ ban + age + I(age**2) + education + afam + hispanic + gender\", data=data).fit()\n",
    "\n",
    "# --- Data for Predictions ---\n",
    "\n",
    "# Data for male without ban\n",
    "dataMaleNoBan = pd.DataFrame({\n",
    "    'ban': [0],          # no ban\n",
    "    'age': [30],         # age\n",
    "    'I(age ** 2)': [30**2],  # age^2\n",
    "    'education': ['college'],  # college degree (reference is 'hs drop out')\n",
    "    'afam': [0],        # african-american\n",
    "    'hispanic': [0],    # hispanic\n",
    "    'gender': ['male']    # male\n",
    "})\n",
    "\n",
    "# Data for male with ban\n",
    "dataMaleBan = dataMaleNoBan.copy()  #  Make a copy to avoid modifying the original\n",
    "dataMaleBan['ban'] = 1\n",
    "\n",
    "# Data for female without ban\n",
    "dataFemaleNoBan = dataMaleNoBan.copy()\n",
    "dataFemaleNoBan['gender'] = 'female'\n",
    "\n",
    "# Data for female with ban\n",
    "dataFemaleBan = dataMaleBan.copy()\n",
    "dataFemaleBan['gender'] = 'female'\n",
    "\n",
    "# --- Prediction Function (invlogit) ---\n",
    "\n",
    "def invlogit(betas, data):\n",
    "  \"\"\"Calculates the predicted probability from a logistic regression.\n",
    "\n",
    "  Args:\n",
    "      betas: A list or array of coefficients (including the intercept).\n",
    "      data:  A Pandas DataFrame with the covariate values.  Must have the\n",
    "             same columns as the data used to fit the model.\n",
    "  Returns:\n",
    "      The predicted probability (a float between 0 and 1).\n",
    "  \"\"\"\n",
    "  # Convert the data and betas into the right format for matrix multiplication\n",
    "  exog = pd.get_dummies(data, drop_first=True) #Handle categorical variables\n",
    "  #Ensure all columns are accounted for\n",
    "  exog = exog.reindex(columns=logit_model.params.index.drop('Intercept'), fill_value=0)\n",
    "\n",
    "  linear_combination = betas[0] + np.dot(betas[1:], exog.values.T)\n",
    "  pY = np.exp(linear_combination) / (1 + np.exp(linear_combination))\n",
    "  return pY[0]  # Return the scalar probability\n",
    "\n",
    "\n",
    "# --- Computing Probabilities ---\n",
    "\n",
    "betas = logit_model.params.values  # Get the coefficients as a NumPy array\n",
    "\n",
    "pMaleBan = invlogit(betas, dataMaleBan)\n",
    "pMaleNoBan = invlogit(betas, dataMaleNoBan)\n",
    "pFemaleBan = invlogit(betas, dataFemaleBan)\n",
    "pFemaleNoBan = invlogit(betas, dataFemaleNoBan)\n",
    "\n",
    "# Calculating changes in probabilities\n",
    "effectForMales = pMaleBan - pMaleNoBan\n",
    "effectForFemales = pFemaleBan - pFemaleNoBan\n",
    "\n",
    "# --- Creating DataFrame for Table Output ---\n",
    "\n",
    "pData = pd.DataFrame({\n",
    "    'male': [pMaleBan, pMaleNoBan, effectForMales],\n",
    "    'female': [pFemaleBan, pFemaleNoBan, effectForFemales]\n",
    "})\n",
    "pData = (pData * 100).round(4)  # Convert to percentage and round\n",
    "pData.index = [\"Ban\", \"No Ban\", \"Difference\"]\n",
    "\n",
    "# --- Displaying the Table (using Markdown for simplicity) ---\n",
    "\n",
    "# Create the Markdown table string\n",
    "table_md = \"|               | Male     | Female   |\\n\"\n",
    "table_md += \"|:--------------|:---------|:---------|\\n\"\n",
    "for row in pData.index:\n",
    "    table_md += f\"| {row}          | {pData.loc[row, 'male']:.4f} | {pData.loc[row, 'female']:.4f} |\\n\"\n",
    "\n",
    "footnote = \"Predicted smoking probabilities for an individual that is 30 years old, \\n with a college degree, that is not african-american, and not hispanic.\"\n",
    "display(Markdown(table_md + \"\\n\\n\" + footnote))\n",
    "\n",
    "#Alternative, but more complicated method using to_markdown()\n",
    "#display(Markdown(pData.to_markdown() + \"\\n\\n\" + footnote))\n",
    "\n",
    "# --- Alternative (using pandas styling for HTML output - more complex)\n",
    "#\n",
    "# styled_#table = pData.style.format(\"{:.4f}\").set_#table_styles([\n",
    "#       {'selector': 'th', 'props': [('text-align', 'left')]}, # Left-align headers\n",
    "#     {'selector': '.row_heading', 'props': [('text-align', 'left')]}, #Left allign index\n",
    "#       {'selector': '', 'props': [('border', '1px solid black')]} # Add borders (optional)\n",
    "#   ]).set_caption(footnote)\n",
    "# display(styled_table)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
