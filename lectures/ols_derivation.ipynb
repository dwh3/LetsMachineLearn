{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions in OLS\n",
    "\n",
    "- Number of observations greater than number of predictors\n",
    "\n",
    "- Each observation is unique.\n",
    "\n",
    "- No multicollinearity (we talked about this with VIF)\n",
    "\n",
    "- Linear model\n",
    "\n",
    "- Homoskedastiity of residuals or equal variances\n",
    "\n",
    "- Errors are distributed normally\n",
    "\n",
    "## Deriving OLS\n",
    "\n",
    "In understanding the assumptions of linear regressions, we can start witht the derivation of OLS (the equation of linear regressions). Linear regressions are ultimately just algebra. In algebra, a big part of it was understanding how $x$ influences $y$. This is what we would find when finding the slope of a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the data\n",
    "x = np.linspace(0, 10, 400)\n",
    "slope = 3\n",
    "intercept = 0\n",
    "y = intercept + slope * x\n",
    "\n",
    "# Define the points (x1, y1) and (x2, y2)\n",
    "x1 = 2\n",
    "y1 = slope * x1\n",
    "x2 = 4\n",
    "y2 = slope * x2\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, color=\"orange\")\n",
    "ax.plot([x1, x2], [y1, y2], 'ro')  # 'ro' for red circles\n",
    "ax.plot([x1, x2], [y1, y1], 'r--', alpha=0.5)  # Dashed red line\n",
    "ax.plot([x2, x2], [y1, y2], 'r--', alpha=0.5)\n",
    "\n",
    "# Annotations (using LaTeX for delta)\n",
    "ax.text((x1 + x2) / 2, y1 - 1.5, r'$\\Delta x$', color=\"red\")\n",
    "ax.text(x2 + 0.5, (y1 + y2) / 2, r'$\\Delta y$', color=\"red\")\n",
    "ax.text(x1, y1, r'$(x_1, y_1)$', ha='right')  # ha='right' for horizontal alignment\n",
    "ax.text(x2, y2, r'$(x_2, y_2)$', ha='left')\n",
    "\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Slope of a Line\")\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, slope * 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here we can see how y changes (varies) as x changes (varies). This is what we called the slope:\n",
    "\n",
    "$$\\beta=\\frac{\\Delta y}{\\Delta x}$$\n",
    "\n",
    "\n",
    "\n",
    "But how can we actually use this? Well, let's consider a case where we have a claim where the only thing that affects their cost is how many times they fill a certain prescription.\n",
    "\n",
    "$$total \\: cost=\\frac{cost}{fill}\\cdot fills$$\n",
    "\n",
    "\n",
    "\n",
    "This would mean that even if we did not know the price of the drug, we could figure it out, even if we only had the total cost and total number of fills.\n",
    "\n",
    "$$total \\: cost=\\frac{cost}{fills}\\cdot fills$$\n",
    "$$\\frac{total \\: cost}{fills}=\\frac{cost}{fill}$$\n",
    "\n",
    "\n",
    "Which with actual numbers would mean:\n",
    "\n",
    "$$80=\\frac{cost}{fills}\\cdot 4$$\n",
    "$$\\frac{80}{4}=20=\\frac{cost}{fill}$$\n",
    "\n",
    "\n",
    "But we know many thinsg can influence an outcome. So if that person had two prescriptions, then:\n",
    "\n",
    "\n",
    "\n",
    "$$total \\: cost=\\frac{cost_1}{fill_1}\\cdot fills_1+\\frac{cost_2}{fill_2}\\cdot fills_2$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To simplify the language here, we can use the slope notation that we had before, so $\\frac{cost_1}{fill_1}=\\beta_1$, etc. making:\n",
    "\n",
    "\n",
    "$$total \\: cost=\\beta_1 \\cdot fills_1+\\beta_2 \\cdot fills_2$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So we can go back to our example and try to solve it. Let's say we have\n",
    "\n",
    "\n",
    "$$100=4\\beta_1+1 \\beta_2$$\n",
    "\n",
    "What happens if we try to solve for each price? We could write:\n",
    "\n",
    "\n",
    "$$\\beta_2=100-4\\beta_1$$\n",
    "\n",
    "But we cannot get actual values here. That is because we have more unknowns than we have equations. This goes to the first assumption of **Number of observations greater than number of predictors**.\n",
    "\n",
    "\n",
    "\n",
    "That means that if we have two drugs we would like to find the price of, given their prescription counts and total dollars,  we would need at least two different observations for them to figure out their costs. But what if we only have the following two equations.\n",
    "\n",
    "\n",
    "$$100=4\\beta_1+1 \\beta_2$$\n",
    "\n",
    "\n",
    "$$100=4\\beta_1+1 \\beta_2$$\n",
    "\n",
    "\n",
    "\n",
    "Well technically we have two equations, but the second equation does not give us any information that was not already in the first equation. So we don't need just as many equations as we have unknowns, we need at least as many unique equations as we have unknowns. **This is one of the things that makes our second rule of \"Each observation is unique.** **This is also related to our assumption that there is no multicollinearity, but that is more rigorous of a proof, and understanding VIF is more than sufficient for understanding why.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So instead, we can look at their bill in June and July and see:\n",
    "\n",
    "\n",
    "$$100=4\\beta_1+1 \\beta_2$$\n",
    "\n",
    "\n",
    "$$120=4\\beta_1+2 \\beta_2$$\n",
    "\n",
    "\n",
    "So we know we would need to solve one equation to get one variable in terms of the other\n",
    "\n",
    "\n",
    "$$\\beta_2=100-4\\beta_1$$\n",
    "\n",
    "\n",
    ", and then plug that into the other equation.....\n",
    "\n",
    "\n",
    "$$120=4\\beta_1+2(100-4\\beta_1)$$\n",
    "$$120=4\\beta_1+200-8\\beta_1$$\n",
    "$$4\\beta_1=80$$\n",
    "$$\\beta_1=20$$\n",
    "\n",
    "Which we plug back into the original equation to get:\n",
    "\n",
    "$$\\beta_2=100-4\\beta_1=100-4(20)=20$$\n",
    "\n",
    "\n",
    "\n",
    "So with 1 equation and 1 unknown, it was 1 step. With 2 equations and 2 unknowns, we get many more steps. This continues exponentially as we add more equations and more unknowns. **So what can we do?**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To figure that out, we can revisit the case with one variable. We said if we want to know the slope, we would do:\n",
    "\n",
    "$$y=x \\beta$$\n",
    "$$\\beta=\\frac{y}{x}$$\n",
    "\n",
    "But this skips some steps. We actually divide each side by x:\n",
    "\n",
    "$$\\frac{1}{x}\\beta x=y\\frac{1}{x}$$\n",
    "\n",
    "\n",
    "\n",
    "But dividing by a number is the same as multiplying by that number to the negative 1 power, meaning:\n",
    "\n",
    "\n",
    "$$\\frac{1}{x}\\beta x=y\\frac{1}{x} \\implies x^{-1}\\beta x=x^{-1}y$$\n",
    "\n",
    "\n",
    "This gives $\\beta=x^{-1}y$ since **$x^{-1}x=1$**.\n",
    "\n",
    "\n",
    "So how does that helps us? Well we are actually able to write out multiple equations in a way that can do that, which means we can write our equations:\n",
    "\n",
    "$$100=4\\beta_1+1 \\beta_2$$\n",
    "\n",
    "\n",
    "$$120=4\\beta_1+2 \\beta_2$$\n",
    "\n",
    "\n",
    "\n",
    "can be written in matrix notation as:\n",
    "\n",
    "$$\n",
    "\\overset{y}{\\begin{bmatrix}\n",
    "    100 \\\\\n",
    "    120\n",
    "\\end{bmatrix}}\n",
    "=\n",
    "\\overset{X}{\\begin{bmatrix}\n",
    "    1 & 4  \\\\\n",
    "    2 & 4\n",
    "\\end{bmatrix}}\n",
    "\\overset{\\beta}{\\begin{bmatrix}\n",
    "    \\beta_1 \\\\\n",
    "    \\beta_2\n",
    "\\end{bmatrix}}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So now whether we have 1 equation and 1 unknown, or 50 equations and 50 unknowns, we can write them as:\n",
    "\n",
    "\n",
    "$\\beta=x^{-1}y$\n",
    "\n",
    "**This is what is meant in the assumption of \"Linear Model\", that the expression can be written as a linear system of equations.**\n",
    "\n",
    "\n",
    "\n",
    "## Matrix multiplication\n",
    "\n",
    "\n",
    "But let's look at our matrix. Since we said:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\begin{aligned}\n",
    "        100 &= 4\\beta_1 + 1\\beta_2 \\\\\n",
    "        120 &= 4\\beta_1 + 2\\beta_2\n",
    "    \\end{aligned}\n",
    "    & \\implies\n",
    "    & \\begin{bmatrix}\n",
    "        100 \\\\\n",
    "        120\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        4 & 1 \\\\\n",
    "        4 & 2\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\beta_1 \\\\\n",
    "        \\beta_2\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This is where the explanation starts to get a little too loose. Tighten it up a bit**\n",
    "\n",
    "This means that we do a dot product where each row of $x$ has a dot product with the column of B.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\begin{aligned}\n",
    "        100 &= 4\\beta_1 + 1\\beta_2 \\\\\n",
    "        120 &= 4\\beta_1 + 2\\beta_2\n",
    "    \\end{aligned}\n",
    "    & \\implies\n",
    "    & \\begin{bmatrix}\n",
    "        \\color{red}{100} \\\\\n",
    "        120\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "        \\color{blue}{4} & \\color{blue}{1} \\\\\n",
    "        4 & 2\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "        \\color{green}{\\beta_1} \\\\\n",
    "        \\color{green}{\\beta_2}\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Below, we highlight the matrix multiplication step-by-step:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\color{red}{100} \\\\\n",
    "    120\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    \\color{blue}{4} & \\color{blue}{1} \\\\\n",
    "    4 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    \\color{green}{\\beta_1} \\\\\n",
    "    \\color{green}{\\beta_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, <span style=\"color:red;\">100</span> is the dot product of the first row of <span style=\"color:blue;\">X = [4, 1]</span> and the column of <span style=\"color:green;\">β = [β<sub>1</sub>, β<sub>2</sub>]</span>:\n",
    "\n",
    "$$\n",
    "\\color{red}{100} = \\color{blue}{4} \\cdot \\color{green}{\\beta_1} + \\color{blue}{1} \\cdot \\color{green}{\\beta_2}\n",
    "$$\n",
    "\n",
    "Likewise, \\(\\color{red}{120}\\) is the dot product of the second row of <span style=\"color:blue;\">X = [4, 2]</span> and the column of <span style=\"color:green;\">β = [β<sub>1</sub>, β<sub>2</sub>]</span>:\n",
    "\n",
    "$$\n",
    "\\color{red}{120} = \\color{blue}{4} \\cdot \\color{green}{\\beta_1} + \\color{blue}{2} \\cdot \\color{green}{\\beta_2}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This means to be able to do matrix multiplication, we need the same number of columns in $X$ as rows in $\\beta$. We write the dimensions of a matrix like so:\n",
    "\n",
    "\n",
    "To perform matrix multiplication, the number of columns in matrix $X$ must be equal to the number of rows in vector $\\beta$. This requirement ensures that each element in the resulting product is a sum of products, calculated by taking the dot product of rows from $X$ with columns from $\\beta$.\n",
    "\n",
    "We write the dimensions of a matrix as follows:\n",
    "\n",
    "- $X$ is an $n \\times k$ matrix, where $n$ is the number of observations (rows) and $k$ is the number of predictors including the intercept term (columns).\n",
    "- $\\beta$ is a $k \\times 1$ vector.\n",
    "- The resulting product $y$ is an $n \\times 1$ vector.\n",
    "\n",
    "Here’s how we denote the dimensions:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    y_n\n",
    "\\end{bmatrix}_{n \\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n",
    "    x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    x_{n1} & x_{n2} & \\cdots & x_{nk}\n",
    "\\end{bmatrix}_{n \\times k}\n",
    "\\begin{bmatrix}\n",
    "    \\beta_1 \\\\\n",
    "    \\beta_2 \\\\\n",
    "    \\vdots \\\\\n",
    "    \\beta_k\n",
    "\\end{bmatrix}_{k \\times 1}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Dimensions of Matrices in Matrix Multiplication\n",
    "\n",
    "In matrix algebra, the order of multiplication matters. Specifically, the number of columns in $X$ must equal the number of rows in $\\beta$. We can visualize this by writting the dimensions, such as $X$ which has $n$ rows, and $k$ columns as:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    " & X  \\\\\n",
    "& \\left[ n \\times k \\right]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "And since the number of columns in $X$ must equal the number of rows in $\\beta$, that means that when we write an equation, the inner dimensions much match. And the result takes the outter dimensions.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{cccc}\n",
    "\\ & y & = & X \\beta \\\\\n",
    "& \\left[ \\color{red}{n} \\times \\color{red}{1} \\right] & & \\left[ \\color{red}{n} \\times \\color{blue}{k} \\right] \\cdot \\left[ \\color{blue}{k} \\times \\color{red}{1} \\right] \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Inverse Rules\n",
    "\n",
    "So the equation $\\beta=\\frac{y}{x}$ again skips steps. The steps are actually\n",
    "\n",
    "We said that $x^{-1}x=1$. The term $x^{-1}$ is known as the inverse of $x$. We won't get into why, but the inverse of a matrix is only defined is that matrix is square. Which means if $x$ has the following dimensions:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    " & X  \\\\\n",
    "& \\left[ n \\times k \\right]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then $n$ needs to equal $k$ to be invertible.\n",
    "\n",
    "\n",
    "So why do we care about that? Well if we think about our data, we could have 1,000,000 observations, and only 50 things that effect our cost. This would mean that our matrix of coefficients would be:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    " & X  \\\\\n",
    "& \\left[ 1,000,000 \\times 50 \\right]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "And here $n \\neq k$, which means there is no such thing as $x^{-1}$. So what can we do?\n",
    "\n",
    "\n",
    "Well let's go back to the drawing board. Let's think of the something like:\n",
    "\n",
    "\n",
    "\n",
    "$$4x=8$$\n",
    "\n",
    "We said before that we would multiply by the inverse of 4 to solve this:\n",
    "\n",
    "\n",
    "$$(4)^{-1}4x=(4)^{-1}8 \\implies 1x=(4)^{-1}8$$\n",
    "\n",
    "\n",
    "\n",
    "But 4 can be decomposed. So instead we could write:\n",
    "\n",
    "$$4x=8 \\implies (2 \\times 2)x=8$$\n",
    "\n",
    "\n",
    "Then our inverse would be:\n",
    "\n",
    "\n",
    "\n",
    "$$(2 \\times 2)x=8 \\implies (2 \\times 2)^{-1}(2 \\times 2)x=(2 \\times 2)^{-1}8 \\implies 1x=(2 \\times 2)^{-1}8$$\n",
    "\n",
    "\n",
    "\n",
    "So how does that help us? Well it tips us off that we can modify our equation to give us something that is invertible. Lucky for us there is an operation known as the transpose, which we would write the transpose of $x$ as $x'$, which takes the rows of the initial Matrix, and turns them into the columns of the transpose. This would mean if we had:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    " & X  \\\\\n",
    "& \\left[ 1,000,000 \\times 50 \\right]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "\\begin{array}{c}\n",
    " & X'  \\\\\n",
    "& \\left[ 50 \\times 1,000,000 \\right]\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "So why would we care about this? Well if we multiply them, the result would be square:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    " & X' X \\\\\n",
    " & \\left[ 50 \\times 50 \\right] \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "This is necessary to be able to invert this. So now to solve for our prices, given that we have more observations than unknowns, we would do:\n",
    "\n",
    "$$y=x \\beta$$\n",
    "$$x'y=x'x \\beta$$\n",
    "$$(x'x)^{-1}x'y=(x'x)^{-1}x'x \\beta$$\n",
    "$$(x'x)^{-1}x'y=\\beta$$\n",
    "\n",
    "\n",
    "\n",
    "**This is OLS.....linear regression**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Stochastic processes\n",
    "\n",
    "\n",
    "\n",
    "But in the real world, there will always be some noise in our data. So instead of the world being generated by:\n",
    "\n",
    "$$y=x \\beta$$\n",
    "\n",
    "We have some noise, which we call the error term:\n",
    "\n",
    "\n",
    "\n",
    "$$y=x \\beta + \\epsilon$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility (equivalent to set.seed(1) in R)\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 1000  # number of observations\n",
    "x = np.random.randn(n)  # Explanatory/Independent Variable (randn for standard normal)\n",
    "epsilon = np.random.randn(n)  # Error term\n",
    "\n",
    "beta0 = 10  # Intercept parameter\n",
    "beta1 = 2  # Slope Parameter\n",
    "\n",
    "# Linear model\n",
    "y = beta0 + beta1 * x + epsilon\n",
    "\n",
    "# Data frame (using pandas for consistency with common Python data analysis)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Graph (using matplotlib, with seaborn-style theme for aesthetics)\n",
    "fig, ax = plt.subplots() #figsize is optional\n",
    "ax.scatter(df['x'], df['y'], s=10) # s controls point size, can be removed\n",
    "ax.set_xlabel(\"x\") #No need for latex here\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Scatter Plot of x vs y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So when we run OLS, we would actually only be able to guess the value of $\\beta$, which we call our estimate:\n",
    "\n",
    "$$\\hat{\\beta}=(x'x)^{-1}x'y$$\n",
    "\n",
    "\n",
    "\n",
    "But what about $\\epsilon$? Well this relates to the conditions that make OLS BLUE.\n",
    "\n",
    "If we use the OLS form from before, we get:\n",
    "\n",
    "$$y=x \\beta + \\epsilon$$\n",
    "$$y - \\epsilon= x \\beta $$\n",
    "$$x'(y - \\epsilon)= x'x \\beta $$\n",
    "$$x'y - x'\\epsilon= x'x \\beta $$\n",
    "$$(x'x)^{-1}(x'y - x'\\epsilon)= (x'x)^{-1}x'x \\beta $$\n",
    "$$(x'x)^{-1}(x'y - x'\\epsilon)= \\hat{\\beta} $$\n",
    "\n",
    "\n",
    "\n",
    "We still see the $\\epsilon$ here. But let's look at another property of matrices, which is orthogonality. A vector can represent a list of numbers, such as a list of the total dollars spent on a claim. For example, consider the vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix}\n",
    "500 \\\\\n",
    "750 \\\\\n",
    "200 \\\\\n",
    "450 \\\\\n",
    "300\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{v}$ represents the total dollars spent on five different claims. \n",
    "\n",
    "But it can also mean direction in a certain dimension such as a two dimensional vector can represent the direction of a line in the x and y directions.\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix}\n",
    "x-direction \\\\\n",
    "y-direction\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "So how is that relevant? Well let's consider the following two lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create the vectors\n",
    "vector_x = [1, 0]\n",
    "vector_y = [0, 1]\n",
    "\n",
    "# Create the data frame for the vectors\n",
    "vectors = pd.DataFrame({\n",
    "    'x': [0, 0],\n",
    "    'y': [0, 0],\n",
    "    'xend': [vector_x[0], vector_y[0]],\n",
    "    'yend': [vector_x[1], vector_y[1]],\n",
    "    'label': [\"Vector 1\", \"Vector 2\"],\n",
    "    'color': [\"blue\", \"red\"]\n",
    "})\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Use a loop to plot each vector, to handle the arrow and color correctly\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(\"\",\n",
    "                xy=(vectors['xend'][i], vectors['yend'][i]),\n",
    "                xytext=(vectors['x'][i], vectors['y'][i]),\n",
    "                arrowprops=dict(arrowstyle=\"->\", color=vectors['color'][i], linewidth=2), #linewidth for arrow size\n",
    "                )\n",
    "    ax.text(vectors['xend'][i], vectors['yend'][i], vectors['label'][i],\n",
    "            va='bottom', ha='left', color='black') #va and ha for text positioning\n",
    "\n",
    "\n",
    "ax.axhline(0, color=\"grey\", linestyle=\"--\")  # Horizontal line\n",
    "ax.axvline(0, color=\"grey\", linestyle=\"--\")  # Vertical line\n",
    "\n",
    "ax.set_xlim(-1, 2)\n",
    "ax.set_ylim(-1, 2)\n",
    "ax.set_title(\"Orthogonal Vectors\")\n",
    "ax.set_xlabel(\"X direction\")\n",
    "ax.set_ylabel(\"Y direction\")\n",
    "\n",
    "# Remove grid lines (equivalent to theme(panel.grid = element_blank()))\n",
    "ax.grid(False)\n",
    "# Good practice to remove the axis spines as well, for a cleaner \"minimal\" look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False) # Optionally keep bottom/left\n",
    "# ax.spines['left'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, we see that a change in the first vector is not associated with a change in the second vector. In fact, you could move as far as you would like in the y-direction, and it never changes your location in the x direction. So the variance of one vector is not correlated with the variance in the other vector. So we can see that intuitively, but what does that mean in terms of math? Well vector one is:\n",
    "\n",
    "$$\n",
    "\\mathbf{v_1} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the other is:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{v_2} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "To find the dot product of these two vectors, we multiply corresponding entries and sum the results:\n",
    "\n",
    "$$\n",
    "\\mathbf{v_1} \\cdot \\mathbf{v_2} = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "0 \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "= (1 \\times 0) + (0 \\times 1) = 0 + 0 = 0\n",
    "$$\n",
    "\n",
    "Since the dot product is zero, this confirms that the vectors are orthogonal. In mathematical terms, orthogonal vectors have a dot product of zero, which means they are perpendicular to each other in the vector space.\n",
    "\n",
    "\n",
    "This means that if variance in one variable is uncorrelated with variance in other variable, then the dot product is 0. So how does that help us? Well going back to our equation.\n",
    "\n",
    "\n",
    "\n",
    "$$(x'x){-1}(x'y - x'\\epsilon)= \\hat{\\beta} $$\n",
    "\n",
    "If our the variance of our input is not correlated with our errors, that would mean that $x$ is orthogonal with $\\epsilon$, which means that $x'\\epsilon=0$, which means that:\n",
    "\n",
    "$$(x'x){-1}(x'y - x'\\epsilon)= \\hat{\\beta} \\implies (x'x){-1}(x'y - 0)= \\hat{\\beta}  \\implies (x'x){-1}x'y= \\hat{\\beta}  $$\n",
    "\n",
    "\n",
    "And and since we said that without any error terms that the true causal influence is:\n",
    "\n",
    "$$(x'x)^{-1}x'y=\\beta$$\n",
    "\n",
    "And\n",
    "\n",
    "$$\\hat{\\beta}=(x'x){-1}x'y$$\n",
    "\n",
    "This means that if x and epsilon are uncorrelated, then our estimate equals the actual  value:\n",
    "\n",
    "\n",
    "$$\\hat{\\beta}=\\beta=(x'x){-1}x'y$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**This is where our assumption of homoskedasticity comes in**. Skedasticity refers to the correlation in the variance of the error term, with respect to the x variable. Homoskedastic means that the variance of the error term is the same for all values of x which would be equaivalent to $x'\\epsilon = 0$. Whereas heteroskedasticity means that the variance of the error term is correlated with x, making $x'\\epsilon \\neq 0$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Number of observations\n",
    "n = 1000\n",
    "\n",
    "# Explanatory/Independent Variable\n",
    "x = np.random.randn(n)\n",
    "\n",
    "# Homoskedastic error term\n",
    "epsilon_homo = np.random.randn(n)\n",
    "\n",
    "# Heteroskedastic error term (error increases with x)\n",
    "epsilon_hetero = np.random.randn(n) * np.abs(x)  # Multiply by abs(x) for heteroskedasticity\n",
    "\n",
    "# Parameters for the linear model\n",
    "beta0 = 10  # Intercept parameter\n",
    "beta1 = 2  # Slope Parameter\n",
    "\n",
    "# Linear model for homoskedastic errors\n",
    "y_homo = beta0 + beta1 * x + epsilon_homo\n",
    "\n",
    "# Linear model for heteroskedastic errors\n",
    "y_hetero = beta0 + beta1 * x + epsilon_hetero\n",
    "\n",
    "# Data frames\n",
    "df_homo = pd.DataFrame({'x': x, 'y': y_homo})\n",
    "df_hetero = pd.DataFrame({'x': x, 'y': y_hetero})\n",
    "\n",
    "# Create subplots (for side-by-side plots)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # 1 row, 2 columns, adjust figsize as needed\n",
    "\n",
    "# Plot for homoskedastic errors\n",
    "axes[0].scatter(df_homo['x'], df_homo['y'], s=10)\n",
    "axes[0].set_title(\"Homoskedastic Errors\")\n",
    "axes[0].set_xlabel(\"x\")\n",
    "axes[0].set_ylabel(\"y\")\n",
    "\n",
    "# Plot for heteroskedastic errors\n",
    "axes[1].scatter(df_hetero['x'], df_hetero['y'], s=10)\n",
    "axes[1].set_title(\"Heteroskedastic Errors\")\n",
    "axes[1].set_xlabel(\"x\")\n",
    "axes[1].set_ylabel(\"y\")\n",
    "\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout() # Prevents labels from overlapping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Error terms are distributed normally\n",
    "\n",
    "\n",
    "In OLS, we can go back to our statistics. We have a null hypothesis, which we need to reject or fail to reject. In doing this, we often have our null hypothesis that something has no effect. Then we see the mean $\\hat{\\beta}$. Then we construct a 95\\% Confidence Interval, where we essentially say we know the true population mean $\\hat{\\beta}$ is somewhere in that range.This is what we are doing in OLS, we are essentially finding the mean correlation in variances, and see if the point estimate is statistically significant from zero. For these tests to be meaningful, we need the error terms to be distributed normally. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Number of observations\n",
    "n = 100\n",
    "\n",
    "# Explanatory/Independent Variable\n",
    "x = np.random.randn(n)\n",
    "\n",
    "# Error term (increased standard deviation)\n",
    "epsilon = np.random.normal(0, 25, n)  # Increased standard deviation\n",
    "\n",
    "# Parameters for the linear model\n",
    "beta0 = 10  # Intercept parameter\n",
    "beta1 = 5  # Slope Parameter\n",
    "\n",
    "# Linear model\n",
    "y = beta0 + beta1 * x + epsilon\n",
    "\n",
    "# Create a DataFrame (required for statsmodels)\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Fit the linear model (using statsmodels.formula.api, which is similar to R's lm)\n",
    "model = sm.ols(\"y ~ x\", data=df).fit()  # 'y ~ x' is the formula, like in R\n",
    "\n",
    "# Get the confidence intervals\n",
    "conf_intervals = model.conf_int()\n",
    "\n",
    "# Extract the estimated coefficients and confidence intervals (into a DataFrame)\n",
    "estimates = pd.DataFrame({\n",
    "    'term': conf_intervals.index,\n",
    "    'estimate': model.params,\n",
    "    'conf.low': conf_intervals[0],  # Lower bound\n",
    "    'conf.high': conf_intervals[1]  # Upper bound\n",
    "})\n",
    "\n",
    "# Plot the confidence intervals\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the points (estimates)\n",
    "ax.plot(estimates['term'], estimates['estimate'], 'o', color='C0') # C0 is the default blue\n",
    "\n",
    "# Plot the error bars (confidence intervals)\n",
    "ax.errorbar(estimates['term'], estimates['estimate'],\n",
    "            yerr=[estimates['estimate'] - estimates['conf.low'],\n",
    "                  estimates['conf.high'] - estimates['estimate']],\n",
    "            fmt='none',  # Don't plot points again\n",
    "            ecolor='C0',  # Use the same color as the points\n",
    "            capsize=5,    # Add caps to the error bars\n",
    "            elinewidth=1)\n",
    "\n",
    "# Add a horizontal line at y=0\n",
    "ax.axhline(0, linestyle='--', color='red')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Coefficient\")\n",
    "ax.set_ylabel(\"Estimate\")\n",
    "ax.set_title(\"95% Confidence Intervals for Coefficients\")\n",
    "\n",
    "# Remove grid lines\n",
    "ax.grid(False)\n",
    "\n",
    "# Add text labels for estimate, lower and upper bounds (using a loop)\n",
    "for i in range(len(estimates)):\n",
    "    ax.text(estimates['term'][i], estimates['estimate'][i],\n",
    "            f\"{estimates['estimate'][i]:.2f}\",\n",
    "            ha='center', va='bottom')\n",
    "    ax.text(estimates['term'][i], estimates['conf.low'][i],\n",
    "            f\"{estimates['conf.low'][i]:.2f}\",\n",
    "            ha='center', va='top')\n",
    "    ax.text(estimates['term'][i], estimates['conf.high'][i],\n",
    "            f\"{estimates['conf.high'][i]:.2f}\",\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Remaining\n",
    "\n",
    "\n",
    "\n",
    "- Constant term is average, the common variance and whatever is not included in regression and is not collinear with a regressor."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
